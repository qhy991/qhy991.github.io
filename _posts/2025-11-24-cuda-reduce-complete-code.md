---
layout: post
title: "CUDA Reduce: å®Œæ•´ä»£ç å®ç°ä¸æµ‹è¯•æŒ‡å—"
date: 2025-11-24
author: Haiyan Qin
tags: [CUDA, Optimization, C++, Source Code]
reading_time: 10
cover_image: /assets/blog-cuda-reduce-complete.png
excerpt: "æœ¬æ–‡æ¡£æ±‡æ€»äº† Cooperative Groups Demoã€Reduce 5 ä»¥åŠ Reduce 6-8 çš„å®Œæ•´å¯ç¼–è¯‘ä»£ç ã€æµ‹è¯• Harness åŠç¼–è¯‘å‘½ä»¤ï¼Œæ–¹ä¾¿å¼€å‘è€…ç›´æ¥å¤åˆ¶è¿è¡Œã€‚"
---

# CUDA Reduce å®Œæ•´ä»£ç åº“

æœ¬æ–‡æ¡£æ—¨åœ¨æä¾›ä¹‹å‰ç³»åˆ—æ–‡ç« ï¼ˆReduce 0.2, 0.3, 5ï¼‰ä¸­è®¨è®ºçš„æ ¸å¿ƒç®—æ³•çš„**å®Œæ•´ã€å¯ç¼–è¯‘ã€å¯è¿è¡Œ**çš„ä»£ç ç‰ˆæœ¬ã€‚æ‰€æœ‰çš„ä»£ç éƒ½åŒ…å«äº† `main` å‡½æ•°å’Œæµ‹è¯•é€»è¾‘ï¼Œæ‚¨å¯ä»¥ç›´æ¥å¤åˆ¶ä¿å­˜ä¸º `.cu` æ–‡ä»¶å¹¶ä½¿ç”¨ `nvcc` ç¼–è¯‘è¿è¡Œã€‚

---

## 1. Cooperative Groups çº¿ç¨‹ç»„åˆ‡åˆ†æ¼”ç¤º

å¯¹åº”æ–‡ç« ï¼š`CUDA Cooperative Groups: Deep Dive into Thread Hierarchy`

è¿™æ®µä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Cooperative Groups (CG) å°†ä¸€ä¸ªçº¿ç¨‹å—ï¼ˆBlockï¼‰å±‚å±‚åˆ‡åˆ†ä¸º Warp (32), Half-Warp (16), Quarter-Warp (8) ä»¥åŠ Tile (8, 4)ï¼Œå¹¶å±•ç¤ºäº†çº¿ç¨‹åœ¨ä¸åŒå±‚çº§ä¸­çš„ Rank å˜åŒ–ã€‚

### ğŸ“„ æºä»£ç : `cg_demo.cu`

```cpp
#include <stdio.h>
#include <cooperative_groups.h>
#include <stdlib.h>

using namespace cooperative_groups;

// ---------- è®¾å¤‡ç«¯ï¼šæ‰“å°ä¸€ä¸ª tile çš„æ‰€æœ‰å…ƒä¿¡æ¯ ----------
template <int T>
__device__ void
show_tile(const char *tag, thread_block_tile<T> p)
{
    // thread_rank(): å½“å‰çº¿ç¨‹åœ¨æœ¬ tile ä¸­çš„åºå· (0 ~ T-1)
    int rank  = p.thread_rank();        
    // size(): æœ¬ tile çš„æ€»çº¿ç¨‹æ•° (æ’ç­‰äº T)
    int size  = p.size();               
    // meta_group_rank(): æœ¬ tile åœ¨çˆ¶ç»„ä¸­çš„åºå·
    int mrank = p.meta_group_rank();    
    // meta_group_size(): çˆ¶ç»„ä¸­æ€»å…±åŒ…å«äº†å¤šå°‘ä¸ªè¿™æ ·çš„ tile
    int msize = p.meta_group_size();    

    // åªè®©å…¨å±€ç¬¬ 1234567 å·çº¿ç¨‹æ‰“å°ï¼Œé¿å…è¾“å‡ºçˆ†ç‚¸
    // å‡è®¾ gridDimè¶³å¤Ÿå¤§æ¶µç›–æ­¤çº¿ç¨‹
    auto grid = this_grid();
    if (grid.thread_rank() == 1234567) {     
        printf("%s rank in tile %2d size %2d  "
               "meta_rank %2d meta_size %2d  "
               "net_size %3d\n",
               tag, rank, size, mrank, msize, msize * size);
    }
}

// ---------- å…¨å±€å†…æ ¸ï¼šæ¼”ç¤º 5 çº§åµŒå¥—åˆ†åŒº ----------
__global__ void cgwarp(int gid)
{
    // 1. è·å–æ•´ä¸ªç½‘æ ¼å’Œçº¿ç¨‹å—å¥æŸ„
    auto grid   = this_grid();
    auto block  = this_thread_block();

    // 2. ç¬¬ä¸€å±‚åˆ‡åˆ†ï¼šåŸºäº Block åˆ‡åˆ†
    // å°† block åˆ‡åˆ†æˆ 32 çº¿ç¨‹çš„ tile (å³æ ‡å‡† Warp)
    auto warp32 = tiled_partition<32>(block);   
    // å°† block åˆ‡åˆ†æˆ 16 çº¿ç¨‹çš„ tile (åŠ Warp)
    auto warp16 = tiled_partition<16>(block);   
    // å°† block åˆ‡åˆ†æˆ 8 çº¿ç¨‹çš„ tile (1/4 Warp)
    auto warp8  = tiled_partition< 8>(block);   

    // 3. ç¬¬äºŒå±‚åˆ‡åˆ†ï¼šåŸºäº Warp åˆ‡åˆ†
    // æ³¨æ„ï¼šè¿™é‡Œæ˜¯å¯¹ warp32 è¿™ä¸ªå­ç»„ç»§ç»­åˆ‡åˆ†ï¼Œè€Œä¸æ˜¯å¯¹ block åˆ‡åˆ†
    auto tile8  = tiled_partition< 8>(warp32);  
    // 4. ç¬¬ä¸‰å±‚åˆ‡åˆ†ï¼šåŸºäº Tile8 åˆ‡åˆ†
    auto tile4  = tiled_partition< 4>(tile8);   

    if (grid.thread_rank() == gid) {
        printf("warps and sub-warps for thread %d:\n", gid);
        show_tile("warp32", warp32);
        show_tile("warp16", warp16);
        show_tile("warp8 ", warp8);
        show_tile("tile8 ", tile8);
        show_tile("tile4 ", tile4);
    }
}

// ---------- host ----------
int main(int argc, char *argv[])
{
    // é»˜è®¤å¯»æ‰¾ç¬¬ 1234567 å·çº¿ç¨‹
    int gid     = (argc > 1) ? atoi(argv[1]) : 1234567;
    // ç¡®ä¿çº¿ç¨‹æ€»æ•°è¶³å¤Ÿå¤§
    int blocks  = 28800; 
    int threads = 256;

    printf("Target Thread GID: %d\n", gid);
    cgwarp<<<blocks, threads>>>(gid);
    cudaDeviceSynchronize();
    return 0;
}
```

### ğŸ”¨ ç¼–è¯‘ä¸è¿è¡Œå‘½ä»¤

```bash
nvcc -arch=sm_70 -o cg_demo cg_demo.cu
./cg_demo 1234567
```

---

## 2. Reduce 5: æ¨¡æ¿å±•å¼€ä¸ Volatile ä¼˜åŒ–

å¯¹åº”æ–‡ç« ï¼š`CUDA Parallel Reduction: Deep Dive into Reduce 5 Optimization`

è¿™æ˜¯ç»å…¸çš„ Reduce 5 å®ç°ï¼Œä½¿ç”¨äº† C++ æ¨¡æ¿è¿›è¡Œå¾ªç¯å±•å¼€ (Loop Unrolling)ï¼Œå¹¶åˆ©ç”¨ `volatile` å…³é”®å­—åœ¨ Warp å†…éƒ¨è¿›è¡Œéšå¼åŒæ­¥ï¼ˆé’ˆå¯¹æ—§æ¶æ„å…¼å®¹æ€§åŠæŒ‡ä»¤ä¼˜åŒ–ï¼‰ã€‚

### ğŸ“„ æºä»£ç : `reduce5.cu`

```cpp
#include <stdio.h>
#include <cuda_runtime.h>

// è¾…åŠ©å‡½æ•°ï¼šWarp å†…å±•å¼€ (Warp Unrolling)
template <unsigned int blockSize>
__device__ void warpReduce(volatile int *sdata, int tid) {
    if (blockSize >= 64) sdata[tid] += sdata[tid + 32];
    if (blockSize >= 32) sdata[tid] += sdata[tid + 16];
    if (blockSize >= 16) sdata[tid] += sdata[tid + 8];
    if (blockSize >= 8)  sdata[tid] += sdata[tid + 4];
    if (blockSize >= 4)  sdata[tid] += sdata[tid + 2];
    if (blockSize >= 2)  sdata[tid] += sdata[tid + 1];
}

// ä¸» Kernel å‡½æ•°
template <unsigned int blockSize>
__global__ void reduce(int *g_idata, int *g_odata, unsigned int n) {
    extern __shared__ int sdata[];

    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x * (blockSize * 2) + tid;
    unsigned int gridSize = blockSize * 2 * gridDim.x;

    sdata[tid] = 0;
    while (i < n) {
        sdata[tid] += g_idata[i] + g_idata[i + blockSize];
        i += gridSize;
    }
    __syncthreads();

    if (blockSize >= 512) { 
        if (tid < 256) { sdata[tid] += sdata[tid + 256]; } 
        __syncthreads(); 
    }
    if (blockSize >= 256) { 
        if (tid < 128) { sdata[tid] += sdata[tid + 128]; } 
        __syncthreads(); 
    }
    if (blockSize >= 128) { 
        if (tid < 64) { sdata[tid] += sdata[tid + 64]; } 
        __syncthreads(); 
    }

    if (tid < 32) {
        warpReduce<blockSize>(sdata, tid);
    }

    if (tid == 0) g_odata[blockIdx.x] = sdata[0];
}

int main() {
    int N = 1 << 24; // 16M elements
    size_t bytes = N * sizeof(int);
    
    int *h_in = (int*)malloc(bytes);
    // åˆå§‹åŒ–è¾“å…¥ä¸º 1ï¼Œé¢„æœŸç»“æœä¸º N
    for(int i=0; i<N; i++) h_in[i] = 1;

    int *d_in, *d_out;
    cudaMalloc(&d_in, bytes);
    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);

    // Grid é…ç½®
    int blockSize = 256;
    // Reduce 5 æ¯ä¸ªçº¿ç¨‹å¤„ç† 2 ä¸ªå…ƒç´ ï¼Œæ‰€ä»¥ Block è¦†ç›– blockSize*2
    int elementsPerBlock = blockSize * 2;
    int gridSize = (N + elementsPerBlock - 1) / elementsPerBlock;
    
    cudaMalloc(&d_out, gridSize * sizeof(int));

    printf("Running Reduce 5 with N=%d, Grid=%d, Block=%d\n", N, gridSize, blockSize);
    
    // å¯åŠ¨ Kernel
    reduce<256><<<gridSize, blockSize, blockSize * sizeof(int)>>>(d_in, d_out, N);
    
    // æ‹·å›éƒ¨åˆ†å’Œ
    int *h_partial = (int*)malloc(gridSize * sizeof(int));
    cudaMemcpy(h_partial, d_out, gridSize * sizeof(int), cudaMemcpyDeviceToHost);
    
    // CPU ç«¯æœ€ç»ˆæ±‡æ€»
    int gpu_sum = 0;
    for(int i=0; i<gridSize; i++) gpu_sum += h_partial[i];
    
    printf("GPU Sum: %d\n", gpu_sum);
    printf("Expected: %d\n", N);
    printf("Result: %s\n", (gpu_sum == N) ? "PASS" : "FAIL");

    cudaFree(d_in);
    cudaFree(d_out);
    free(h_in);
    free(h_partial);
    return 0;
}
```

### ğŸ”¨ ç¼–è¯‘ä¸è¿è¡Œå‘½ä»¤

```bash
nvcc -arch=sm_70 -o reduce5 reduce5.cu
./reduce5
```

---

## 3. Reduce è¿›åŒ–è®º: ä» CG åˆ° Shuffle å†åˆ° Library

å¯¹åº”æ–‡ç« ï¼š`CUDA Reduction Evolution: From Modern C++ to Extreme Performance`

è¿™ä¸ªæ–‡ä»¶é›†æˆäº†ä¸‰ä¸ªç‰ˆæœ¬çš„ Kernelï¼š
*   **Reduce 6**: ä½¿ç”¨ Cooperative Groups è¯­æ³•é‡å†™ Reduce 5ã€‚
*   **Reduce 7**: ä½¿ç”¨ Warp Shuffle æŒ‡ä»¤ + Atomic Addï¼Œç§»é™¤å…±äº«å†…å­˜ä¾èµ–ã€‚
*   **Reduce 7_v1**: åœ¨ Reduce 7 åŸºç¡€ä¸Šå¢åŠ  `float4` å‘é‡åŒ–åŠ è½½ã€‚
*   **Reduce 8**: ç›´æ¥è°ƒç”¨ NVIDIA å®˜æ–¹ `cooperative_groups/reduce.h` åº“ã€‚

### ğŸ“„ æºä»£ç : `reduce_evolution.cu`

```cpp
#include <stdio.h>
#include <cuda_runtime.h>
#include <cooperative_groups.h>
#include <cooperative_groups/reduce.h>

namespace cg = cooperative_groups;

// ==========================================
// Reduce 6: Cooperative Groups Basic
// ==========================================
__global__ void reduce6(int *g_idata, int *g_odata, unsigned int n) {
    cg::thread_block block = cg::this_thread_block();
    extern __shared__ int sdata[];
    unsigned int tid = block.thread_rank();
    
    // Grid-Stride Loop
    unsigned int i = block.group_index().x * (block.size() * 2) + tid;
    unsigned int gridSize = block.size() * 2 * grid.size();
    
    sdata[tid] = 0;
    while (i < n) {
        sdata[tid] += g_idata[i] + g_idata[i + block.size()];
        i += gridSize;
    }
    block.sync();

    if (block.size() >= 512) { 
        if (tid < 256) { sdata[tid] += sdata[tid + 256]; } 
        block.sync(); 
    }
    if (block.size() >= 256) { 
        if (tid < 128) { sdata[tid] += sdata[tid + 128]; } 
        block.sync(); 
    }
    if (block.size() >= 128) { 
        if (tid < 64) { sdata[tid] += sdata[tid + 64]; } 
        block.sync(); 
    }
    
    if (block.size() >= 64) {
        if (tid < 32) {
             // ç®€å•æ¨¡æ‹Ÿ Reduce 5 çš„ Warp Unrollingï¼Œä½†åœ¨ CG ä¸­æ¨èä½¿ç”¨ Shuffle
             // è¿™é‡Œä¸ºäº†ä¿æŒç»“æ„ä¸€è‡´æ€§ï¼Œå‡è®¾ sdata è¶³å¤Ÿå®‰å…¨
             volatile int* vmem = sdata;
             vmem[tid] += vmem[tid + 32];
             vmem[tid] += vmem[tid + 16];
             vmem[tid] += vmem[tid + 8];
             vmem[tid] += vmem[tid + 4];
             vmem[tid] += vmem[tid + 2];
             vmem[tid] += vmem[tid + 1];
        }
    }

    if (tid == 0) g_odata[block.group_index().x] = sdata[0];
}

// ==========================================
// Reduce 7: Warp Shuffle & Atomic
// ==========================================
template <typename T>
__device__ __forceinline__ T warpReduceSum(cg::thread_block_tile<32> g, T val) {
    val += g.shfl_down(val, 16);
    val += g.shfl_down(val, 8);
    val += g.shfl_down(val, 4);
    val += g.shfl_down(val, 2);
    val += g.shfl_down(val, 1);
    return val;
}

__global__ void reduce7(int *g_idata, int *g_odata, unsigned int n) {
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);

    int sum = 0;
    // Grid-Stride Loop
    unsigned int i = block.group_index().x * block.size() + block.thread_rank();
    unsigned int gridSize = block.size() * grid.size();
    
    while (i < n) {
        sum += g_idata[i];
        i += gridSize;
    }
    
    // Warp Reduce
    sum = warpReduceSum(warp, sum);

    // æ¯ä¸ª Warp çš„ 0 å·çº¿ç¨‹è´Ÿè´£åŸå­ç´¯åŠ 
    if (warp.thread_rank() == 0) {
        atomicAdd(g_odata, sum);
    }
}

// ==========================================
// Reduce 7_v1: Vectorized (float4)
// ==========================================
// æ³¨æ„ï¼šä¸ºäº†æ¼”ç¤º float4 ä¼˜åŠ¿ï¼Œè¿™é‡Œä½¿ç”¨ float ç±»å‹
__global__ void reduce7_v1(float *g_idata, float *g_odata, unsigned int n) {
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);

    float4 v4 = make_float4(0.f, 0.f, 0.f, 0.f);
    
    // å‘é‡åŒ–åŠ è½½å¾ªç¯
    unsigned int tid = block.size() * block.group_index().x + block.thread_rank();
    unsigned int gridSize = grid.size() * block.size();
    
    // æ³¨æ„ï¼šn éœ€è¦èƒ½è¢« 4 æ•´é™¤ï¼Œæˆ–è€…åœ¨è¿™é‡Œå¤„ç†è¾¹ç•Œ
    for (unsigned int idx = tid; idx < n / 4; idx += gridSize) {
        float4 tmp = reinterpret_cast<const float4 *>(g_idata)[idx];
        v4.x += tmp.x;
        v4.y += tmp.y;
        v4.z += tmp.z;
        v4.w += tmp.w;
    }
    
    float sum = v4.x + v4.y + v4.z + v4.w;
    
    // Warp Reduce
    sum = warpReduceSum(warp, sum);

    if (warp.thread_rank() == 0) {
        atomicAdd(g_odata, sum);
    }
}

// ==========================================
// Reduce 8: CG Library
// ==========================================
__global__ void reduce8(int *g_idata, int *g_odata, unsigned int n) {
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);

    int sum = 0;
    unsigned int i = block.group_index().x * block.size() + block.thread_rank();
    unsigned int gridSize = block.size() * grid.size();
    
    while (i < n) {
        sum += g_idata[i];
        i += gridSize;
    }

    // ä½¿ç”¨å®˜æ–¹åº“å‡½æ•°
    sum = cg::reduce(warp, sum, cg::plus<int>());

    if (warp.thread_rank() == 0) {
        atomicAdd(g_odata, sum);
    }
}

int main() {
    int N = 1 << 24; // 16M
    size_t bytes = N * sizeof(int);
    
    // --- Setup for Int kernels (6, 7, 8) ---
    int *h_in = (int*)malloc(bytes);
    for(int i=0; i<N; i++) h_in[i] = 1;
    
    int *d_in, *d_out;
    cudaMalloc(&d_in, bytes);
    cudaMalloc(&d_out, sizeof(int) * 1024); // è¶³å¤Ÿå­˜æ”¾éƒ¨åˆ†å’Œæˆ–åŸå­ç´¯åŠ ç»“æœ
    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);

    int blockSize = 256;
    int gridSize = (N + blockSize - 1) / blockSize;

    // 1. Test Reduce 6
    printf("\n--- Testing Reduce 6 ---\n");
    int gridSizeR6 = (N + (blockSize*2) - 1) / (blockSize*2);
    reduce6<<<gridSizeR6, blockSize, blockSize*sizeof(int)>>>(d_in, d_out, N);
    
    int *h_partial = (int*)malloc(gridSizeR6 * sizeof(int));
    cudaMemcpy(h_partial, d_out, gridSizeR6 * sizeof(int), cudaMemcpyDeviceToHost);
    int sum6 = 0;
    for(int i=0; i<gridSizeR6; i++) sum6 += h_partial[i];
    printf("Reduce 6 Result: %d (Expected: %d) -> %s\n", sum6, N, (sum6==N)?"PASS":"FAIL");
    free(h_partial);

    // 2. Test Reduce 7
    printf("\n--- Testing Reduce 7 ---\n");
    cudaMemset(d_out, 0, sizeof(int)); // åŸå­æ“ä½œå‰æ¸…é›¶
    reduce7<<<gridSize, blockSize>>>(d_in, d_out, N);
    int sum7;
    cudaMemcpy(&sum7, d_out, sizeof(int), cudaMemcpyDeviceToHost);
    printf("Reduce 7 Result: %d (Expected: %d) -> %s\n", sum7, N, (sum7==N)?"PASS":"FAIL");

    // 3. Test Reduce 8
    printf("\n--- Testing Reduce 8 ---\n");
    cudaMemset(d_out, 0, sizeof(int));
    reduce8<<<gridSize, blockSize>>>(d_in, d_out, N);
    int sum8;
    cudaMemcpy(&sum8, d_out, sizeof(int), cudaMemcpyDeviceToHost);
    printf("Reduce 8 Result: %d (Expected: %d) -> %s\n", sum8, N, (sum8==N)?"PASS":"FAIL");

    // --- Setup for Float kernel (7_v1) ---
    printf("\n--- Testing Reduce 7_v1 (Float4) ---\n");
    float *h_in_f = (float*)malloc(N * sizeof(float));
    for(int i=0; i<N; i++) h_in_f[i] = 1.0f;
    float *d_in_f, *d_out_f;
    cudaMalloc(&d_in_f, N * sizeof(float));
    cudaMalloc(&d_out_f, sizeof(float));
    cudaMemcpy(d_in_f, h_in_f, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemset(d_out_f, 0, sizeof(float));
    
    reduce7_v1<<<gridSize, blockSize>>>(d_in_f, d_out_f, N);
    
    float sum7v1;
    cudaMemcpy(&sum7v1, d_out_f, sizeof(float), cudaMemcpyDeviceToHost);
    printf("Reduce 7_v1 Result: %.1f (Expected: %.1f) -> %s\n", sum7v1, (float)N, (sum7v1==N)?"PASS":"FAIL");

    // Cleanup
    cudaFree(d_in); cudaFree(d_out);
    cudaFree(d_in_f); cudaFree(d_out_f);
    free(h_in); free(h_in_f);
    
    return 0;
}
```

### ğŸ”¨ ç¼–è¯‘ä¸è¿è¡Œå‘½ä»¤

```bash
nvcc -arch=sm_70 -o reduce_evolution reduce_evolution.cu
./reduce_evolution
```
